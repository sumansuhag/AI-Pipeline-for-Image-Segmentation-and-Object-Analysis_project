{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57dbdb17-8479-413c-b8e5-ea17e6c00de4",
   "metadata": {},
   "source": [
    "## Image Segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dfabbaf-5d2c-43b3-9a14-2f13b19245e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "def segment_image(image_path):\n",
    "    # Load the pre-trained Mask R-CNN model\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = transform(image_rgb).unsqueeze(0)\n",
    "\n",
    "    # Perform segmentation\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    return predictions, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1bd37-1727-4028-a4d1-75d2c785562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers pytesseract opencv-python matplotlib pillow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490e8fc-3913-4ce5-94ab-32927b069969",
   "metadata": {},
   "source": [
    "## objecyt extraction & storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817f278-6521-4477-85d8-bfb69f149309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def extract_and_save_objects(predictions, original_image, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    objects_info = []\n",
    "    for i, mask in enumerate(predictions[0]['masks']):\n",
    "        if mask.shape[0] > 0:  # Ensure mask is not empty\n",
    "            mask_np = mask[0].mul(255).byte().cpu().numpy()\n",
    "            object_image = cv2.bitwise_and(original_image, original_image, mask=mask_np)\n",
    "            object_path = os.path.join(output_dir, f'object_{i}.png')\n",
    "            cv2.imwrite(object_path, object_image)\n",
    "            objects_info.append({\n",
    "                'id': i,\n",
    "                'path': object_path\n",
    "            })\n",
    "    \n",
    "    return objects_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66e1cc-9170-4297-bddc-2a0db03f003d",
   "metadata": {},
   "source": [
    "## Object identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cb436-8536-4523-b536-5125585f5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def identify_objects(object_images):\n",
    "    classifier = pipeline(\"image-classification\")\n",
    "    descriptions = []\n",
    "\n",
    "    for img_info in object_images:\n",
    "        try:\n",
    "            result = classifier(img_info['path'])\n",
    "            descriptions.append({\n",
    "                'id': img_info['id'],\n",
    "                'description': result[0]['label'] if result else \"Unknown\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_info['path']}: {str(e)}\")\n",
    "            descriptions.append({\n",
    "                'id': img_info['id'],\n",
    "                'description': \"Error in processing\"\n",
    "            })\n",
    "\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da19008f-2a20-4f25-8b7a-81bf59c9b2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b15d44-1f2b-416d-bf89-18d4eac7868c",
   "metadata": {},
   "source": [
    "## Text/Data Extraction from Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f45b36-4346-42e5-98f0-d3495c6f6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {image_path}: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7ec83-9d73-4aa6-b7de-780c9f290957",
   "metadata": {},
   "source": [
    "## Summarize Object Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099979d-56c3-4005-8a7b-406b4f35061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "   from transformers import pipeline\n",
    "\n",
    "def summarize_attributes(descriptions):\n",
    "    summarizer = pipeline(\"summarization\")\n",
    "    summaries = []\n",
    "\n",
    "    for desc in descriptions:\n",
    "        try:\n",
    "            input_text = desc['description'][:1024]  # Limit input length\n",
    "            summary = summarizer(input_text, max_length=50, min_length=10, do_sample=False)\n",
    "            summaries.append({\n",
    "                'id': desc['id'],\n",
    "                'summary': summary[0]['summary_text']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error summarizing description: {str(e)}\")\n",
    "            summaries.append({\n",
    "                'id': desc['id'],\n",
    "                'summary': \"Error in summarization\"\n",
    "            })\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47232ce6-77a6-49e2-a3b5-6b866d8ea6d3",
   "metadata": {},
   "source": [
    "## Data Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d60cb7c-b4e8-4e86-9e59-53949c6a4745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def map_data(objects_info, descriptions, extracted_texts, summaries):\n",
    "    mapped_data = []\n",
    "    \n",
    "    for obj in objects_info:\n",
    "        obj_id = obj['id']\n",
    "        mapped_obj = {\n",
    "            \"id\": obj_id,\n",
    "            \"path\": obj['path'],\n",
    "            \"description\": next((d['description'] for d in descriptions if d['id'] == obj_id), \"\"),\n",
    "            \"extracted_text\": next((t for t in extracted_texts if t['id'] == obj_id), {}).get('text', \"\"),\n",
    "            \"summary\": next((s['summary'] for s in summaries if s['id'] == obj_id), \"\")\n",
    "        }\n",
    "        mapped_data.append(mapped_obj)\n",
    "    \n",
    "    try:\n",
    "        with open('mapped_data.json', 'w') as f:\n",
    "            json.dump(mapped_data, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing mapped data to file: {str(e)}\")\n",
    "    \n",
    "    return mapped_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13e635-8e20-4793-a2d0-012e172b1eed",
   "metadata": {},
   "source": [
    "## Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58125ae5-22cc-44f3-8609-1af1485ae26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_output_image(original_image_path, mapped_data):\n",
    "    try:\n",
    "        original_image = cv2.imread(original_image_path)\n",
    "        original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(original_image_rgb)\n",
    "        \n",
    "        for obj in mapped_data:\n",
    "            x, y = 10, 10  # You may need to adjust this based on actual object positions\n",
    "            plt.text(x, y, f\"ID: {obj['id']}\\n{obj['description'][:20]}...\", \n",
    "                     color='red', fontsize=8, bbox=dict(facecolor='white', alpha=0.7))\n",
    "            y += 40  # Move text position for next object\n",
    "        \n",
    "        plt.title(\"Annotated Image with Object Descriptions\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('output_with_annotations.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Output image generated successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating output image: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9adb1-eca9-4795-8acd-8eb46c557423",
   "metadata": {},
   "source": [
    "##  main script integrates of the AI pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01904c9-8ad0-4609-b959-ee9f5b970846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Import all the necessary functions from the modules we created\n",
    "\n",
    "def segment_image(image_path):\n",
    "    # Step 1: Image Segmentation\n",
    "    predictions, original_image = segment_image(image_path)\n",
    "    \n",
    "    # Step 2: Object Extraction and Storage\n",
    "    output_dir = 'segmented_objects'\n",
    "    objects_info = extract_and_save_objects(predictions, original_image, output_dir)\n",
    "    \n",
    "    # Step 3: Object Identification\n",
    "    descriptions = identify_objects(objects_info)\n",
    "    \n",
    "    # Step 4: Text/Data Extraction from Objects\n",
    "    extracted_texts = [{'id': obj['id'], 'text': extract_text_from_image(obj['path'])} for obj in objects_info]\n",
    "    \n",
    "    # Step 5: Summarize Object Attributes\n",
    "    summaries = summarize_attributes(descriptions)\n",
    "    \n",
    "    # Step 6: Data Mapping\n",
    "    mapped_data = map_data(objects_info, descriptions, extracted_texts, summaries)\n",
    "    \n",
    "    # Step 7: Output Generation\n",
    "    generate_output_image(image_path, mapped_data)\n",
    "    \n",
    "    print(\"Pipeline completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"path/to/your/image.jpg\"\n",
    "    segment_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee3c49-817c-491b-8a98-6534b33b5802",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b3e72-ba66-479d-a733-3a5f4bea8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302754a3-1f2d-40a0-80c7-6f38f10658f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b32ec-1fd6-43db-b238-ef5da5aef6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250a60b-d4b2-4fbe-9fff-acdc5da68993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
